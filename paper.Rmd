---
title: "Group 9 Paper"
author: "Henrique Magalhaes Rio , Oriana Meldrum, Jorie Alvis"
date: "12/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(445)

library(ISLR)
library(leaps)
library(tidyr)
library(dplyr)
library(ggplot2)
library(glmnet)
library(caret)
library(pls) 
library(tree)
library(randomForest)
library(knitr)
library(class)
library(fastDummies)
library(reprtree)
library(MASS)
library(e1071)
library(gbm)








train <- read.csv("train.csv")
test <- read.csv("test.csv")


train$ChestPainType <- as.factor(train$ChestPainType)
train$Sex <- as.factor(train$Sex)
train$FastingBS <- as.factor(train$FastingBS)
train$RestingECG <- as.factor(train$RestingECG)
train$ExerciseAngina <- as.factor(train$ExerciseAngina)
train$ST_Slope <- as.factor(train$ST_Slope)
train$HeartDisease <- as.factor(train$HeartDisease)


test$ChestPainType <- as.factor(test$ChestPainType)
test$Sex <- as.factor(test$Sex)
test$FastingBS <- as.factor(test$FastingBS)
test$RestingECG <- as.factor(test$RestingECG)
test$ExerciseAngina <- as.factor(test$ExerciseAngina)
test$ST_Slope <- as.factor(test$ST_Slope)
test$HeartDisease <- as.factor(test$HeartDisease)
```

# Introduction

In 2019 659,041 Americans died of heart disease. Four out of five heart disease deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by heart disease and this dataset we used contains 11 variables that can be used to predict a possible heart disease.

People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors) need early detection and management so machine learning techniques are of great clinical interest for early detection. 

# Methods

## Logistic Regression


```{r,echo=FALSE}
model_glm = glm(HeartDisease ~ ., data = train, family = "binomial")
summary(model_glm)


predicted2 <- predict(model_glm, newdata = test, "response")
predicted2 <- ifelse(predicted2>0.5,1,0)
confm2<-table(predicted=predicted2,True=test$HeartDisease)
confm2

paste("Accuracy :",(confm2[1,1]+confm2[2,2])/sum(confm2))

overall_logit <- (confm2[1,1]+confm2[2,2])/sum(confm2)
```


 We started by fitting a logistic regression model with all the variables, and from the results above we see that there are a few non-significant variables and important ones, it is worth noting that SexM variable is is statistically significant and positive, which interesting as it means that on average men are more likely to get heart disease than women given that the other variables are the same, also, ST_SlopeFlat and ST_SlopeUp are statistically significant and the  show different signs which means that people with ST_SlopeFlat have a higher Likelihood of having heart Disease when compared to those that have ST_SlopeUp, and this is consistent with science as ST_Slope is a important factor in the diagnosis of a heart Disease. Overall, for a simple method that also allows for inference, it performed relatively well with an accuracy of 0.85.
 
 

 

  

## LDA

This will be the section on LDA. Include discussion on how & why you chose to do it the way you did it. 

## KNN


K-nearest Neighbors is useful for both regression and classification. An object will be classified by a vote of the nearest k neighbors. The object is assigned to the class most common to those k neighbors. For this project, k was selected by cross validation.  The best accuracy comes from a k of 23 with and accuracy of 0.6878024. Technically both k=1 and k=4 have higher accuracy, but we are avoiding k=1 using as any outlines would create a skew and in certain types of classification problems, our research shower it was better to avoid using an odd number as this can tie votes. With an prediction rate of only 0.6757493 had the lowest overall correct prediction rate. 

## LASSO

When using the LASSO method, it was important to select family = "binomial" for the classification problem. The tuning lambda value was selected through cross validation. 

## Bagging

This will be the section on Bagging. Include discussion on how & why you chose to do it the way you did it. 

## Bosting



```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")


train$ChestPainType <- as.factor(train$ChestPainType)
train$Sex <- as.factor(train$Sex)
train$FastingBS <- as.factor(train$FastingBS)
train$RestingECG <- as.factor(train$RestingECG)
train$ExerciseAngina <- as.factor(train$ExerciseAngina)
train$ST_Slope <- as.factor(train$ST_Slope)


test$ChestPainType <- as.factor(test$ChestPainType)
test$Sex <- as.factor(test$Sex)
test$FastingBS <- as.factor(test$FastingBS)
test$RestingECG <- as.factor(test$RestingECG)
test$ExerciseAngina <- as.factor(test$ExerciseAngina)
test$ST_Slope <- as.factor(test$ST_Slope)



set.seed(445)




oob = trainControl(method = "oob")
cv_5 = trainControl(method = "cv", number = 5)

gbm_grid =  expand.grid(interaction.depth = 1:5,
                        n.trees = (1:5) * 200,
                        shrinkage = c(0.01,0.015,0.1),
                        n.minobsinnode = 10)


gbm_tune = train(as.factor(HeartDisease) ~ ., data = train,
                      method = "gbm",
                      trControl = cv_5,
                      verbose = FALSE,
                      tuneGrid = gbm_grid)


plot(gbm_tune)
```

This will be the section on Boosting. Include discussion on how & why you chose to do it the way you did it. 

## Random Forest

This will be the section on Random Forest. Include discussion on how & why you chose to do it the way you did it. 

## Representative Tree

This will be the section on Representative Tree. Include discussion on how & why you chose to do it the way you did it. 

## SVM

This will be the section on SVM. Include discussion on how & why you chose to do it the way you did it. 

# Methods That Didn't Work

This will be the section on methods we considered but did work. Include discussion on how & why you didn't chose to do it. 

# Conclusion

## Future Proposal

It would be interesting to re-try this project, removing the ST-Slope value. As we learned from industry professionals, the ST Slope is nearly always associated with some type of heart rhythm anomaly, and thus is frequently present in cases of heart disease. Many of our models indicated the ST Slope as the number one predictor for heart disease due to this strong association. 

