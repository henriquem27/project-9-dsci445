---
title: "Group 9 Paper"
author: "Henrique Magalhaes Rio , Oriana Meldrum, Jorie Alvis"
date: "12/11/2021"
output: pdf_document
---


```{r , include=FALSE}

#installation for the reprtree package.
options(repos='http://cran.rstudio.org')
have.packages <- installed.packages()
cran.packages <- c('devtools','plotrix','randomForest','tree')
to.install <- setdiff(cran.packages, have.packages[,1])
if(length(to.install)>0) install.packages(to.install)

library(devtools)
if(!('reprtree' %in% installed.packages())){
  install_github('araastat/reprtree')
}
for(p in c(cran.packages, 'reprtree')) eval(substitute(library(pkg), list(pkg=p)))



```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(445)

library(ISLR)
library(leaps)
library(tidyr)
library(dplyr)
library(ggplot2)
library(glmnet)
library(caret)
library(pls) 
library(tree)
library(randomForest)
library(knitr)
library(class)
library(fastDummies)
library(reprtree)
library(MASS)
library(e1071)
library(gbm)








train <- read.csv("train.csv")
test <- read.csv("test.csv")


train$ChestPainType <- as.factor(train$ChestPainType)
train$Sex <- as.factor(train$Sex)
train$FastingBS <- as.factor(train$FastingBS)
train$RestingECG <- as.factor(train$RestingECG)
train$ExerciseAngina <- as.factor(train$ExerciseAngina)
train$ST_Slope <- as.factor(train$ST_Slope)
train$HeartDisease <- as.factor(train$HeartDisease)


test$ChestPainType <- as.factor(test$ChestPainType)
test$Sex <- as.factor(test$Sex)
test$FastingBS <- as.factor(test$FastingBS)
test$RestingECG <- as.factor(test$RestingECG)
test$ExerciseAngina <- as.factor(test$ExerciseAngina)
test$ST_Slope <- as.factor(test$ST_Slope)
test$HeartDisease <- as.factor(test$HeartDisease)
```

# Introduction

In 2019 659,041 Americans died of heart disease. Four out of five heart disease deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by heart disease and this dataset we used contains 11 variables that can be used to predict a possible heart disease.

## Motivation

People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors) need early detection and management so machine learning techniques are of great clinical interest for early detection. 

For this project, we proposed to use the various machine learning methods learned this semester in CSU's DSCI 445 Machine Learning class that can be applied to a classification problem.



# Methods

## Logistic Regression


```{r,echo=FALSE}
model_glm = glm(HeartDisease ~ ., data = train, family = "binomial")
summary(model_glm)


predicted2 <- predict(model_glm, newdata = test, "response")
predicted2 <- ifelse(predicted2>0.5,1,0)
confm2<-table(predicted=predicted2,True=test$HeartDisease)
confm2

paste("Accuracy :",(confm2[1,1]+confm2[2,2])/sum(confm2))

overall_logit <- (confm2[1,1]+confm2[2,2])/sum(confm2)
```


 We started by fitting a logistic regression model with all the variables, and from the results above we see that there are a few non-significant variables and important ones, it is worth noting that SexM variable is is statistically significant and positive, which interesting as it means that on average men are more likely to get heart disease than women given that the other variables are the same, also, ST_SlopeFlat and ST_SlopeUp are statistically significant and the  show different signs which means that people with ST_SlopeFlat have a higher Likelihood of having heart Disease when compared to those that have ST_SlopeUp, and this is consistent with science as ST_Slope is a important factor in the diagnosis of a heart Disease. Overall, for a simple method that also allows for inference, it performed relatively well with an accuracy of 0.85.
 
 

 

  

## LDA

Our next method for our binary classification problem was Linear Discriminate Analysis (LDA). LDA is used to find a linear combination of factors to sperate 2 or more classes. Typically, LDA uses only numeric predictors but through dummy variables, LDA can use categorical variables. This allows for limited inferences to be made from LDA.

```{r LDA,echo=FALSE}
LDA_model = lda(HeartDisease~., data=train)
LDA_model

preds = predict(LDA_model, test)
overall_LDA = sum(ifelse(preds$class == test$HeartDisease, 1, 0))/nrow(test)

table(preds$class, "True"=test$HeartDisease)
```

The LDA model achived a test accuracy of `r round(overall_LDA, 3)`. Even though we can't make strong infrences from our LDA, we can look at the means of each factor of each class. We see that the Resting BP factor has little class mean difference. 

## KNN


The next method we used was K-Nearest Neighbors. K-nearest Neighbors is useful for both regression and classification. An object will be classified by a vote of the nearest k neighbors. The object is assigned to the class most common to those k neighbors. For this project, k was selected by cross validation.  The best accuracy comes from a k of 23 with and accuracy of 0.6878024. Technically both k=1 and k=4 have higher accuracy, but we are avoiding k=1 using as any outlines would create a skew and in certain types of classification problems, our research shower it was better to avoid using an odd number as this can tie votes. With an accuracy rate of only 0.6757493 had the lowest overall accuracy rate. 

```{r message=FALSE, warning=FALSE,echo=FALSE}
temp_train <- dummy_cols(train, select_columns = c("Sex", "ChestPainType", "RestingECG", "ST_Slope", "ExerciseAngina") )
temp_test <- dummy_cols(test, select_columns = c("Sex", "ChestPainType", "RestingECG", "ST_Slope", "ExerciseAngina"))
drops <- c("Sex", "ChestPainType", "RestingECG", "ST_Slope", "ExerciseAngina")
temp_train <- temp_train[, !(names(temp_train) %in% drops)]
temp_test <-temp_test[, !(names(temp_test) %in% drops)]
#trainknn <- temp_train[-temp_train$HeartDisease]
#testknn <- test[-test$HeartDisease]
num_k <- seq(1:100)
my_cv <- knn.cv(temp_train, temp_train$HeartDisease, k=num_k)
knn_pred_1 <- knn(temp_train, temp_test, temp_train$HeartDisease, k=11)
confm_knn_1<-table(knn_pred_1,test$HeartDisease)
overall_knn_1 <- (confm_knn_1[1,1]+confm_knn_1[2,2])/sum(confm_knn_1)
overall_knn_1
 #confusion matrix
kable(confm_knn_1)
mean(knn_pred_1 !=test$HeartDisease)
     
```

## LASSO

After KNN, we used  the least absolute shrinkage and selection operator(LASSO). When using the LASSO method, it was important to select family = "binomial" for the classification problem. The tuning lambda value was selected through cross validation. LASSO performed relatively well with an accuracy of 0.85.

```{r message=FALSE, warning=FALSE,echo=FALSE}
train_matrix <- model.matrix(HeartDisease ~ ., data = train)
test_matrix <- model.matrix(HeartDisease ~ ., data = test)
grid <- 10^seq(10, -2, length=100)
lasso <- glmnet(train_matrix, train$HeartDisease, alpha = 1, lambda = grid, thresh = 1e-12, family = "binomial")
#plot(lasso)
cv_lasso  <- cv.glmnet(train_matrix, train[, "HeartDisease"], alpha=1,family = "binomial")
#plot(cv_lasso)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_prediction <- predict(lasso, newx = test_matrix, s = best_lambda_lasso)
lasso_MSE <- mean((lasso_prediction - test[, "HeartDisease"])^2)
num_zeros <-cv_lasso$nzero[which.min(cv_lasso$cvm)]
# MSE values to use:
paste("Tuning Lambda",best_lambda_lasso)
#lasso_MSE
#paste("Number of zeros",num_zeros)
#Confusion Matrix
predicted_lasso <- predict(lasso, newx= test_matrix, s=best_lambda_lasso)
predicted_lasso <- ifelse(predicted_lasso>0.5,1,0)
confm_lasso<-table(predicted=predicted_lasso,True=test$HeartDisease)
confm_lasso
#Over all correct:
overall_lasso <- (confm_lasso[1,1]+confm_lasso[2,2])/sum(confm_lasso)
kable(confm_lasso) #confusion matrix
overall_lasso #over all correct
```



## Bagging

The bagging model was preformed with 500 trees and identified the most important variable as being ST_Slope followed by Chest Pain Type. Interestingly, resting ECG and Resting BP both had negative importance. This means if we remove those two features, we would improve model performance. As other methods found resting ECG and Resting BP to be among the least important variable, it would be worth taking the time to remove them and see if it improves the accuracy rates for our data. Bagging performed rather disappointingly with an accuracy of 0.84, the lowest of all methods except KNN.

```{r,echo=FALSE,warning=FALSE,fig.width=6,fig.height=3}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
train$ChestPainType <- as.factor(train$ChestPainType)
train$Sex <- as.factor(train$Sex)
train$FastingBS <- as.factor(train$FastingBS)
train$RestingECG <- as.factor(train$RestingECG)
train$ExerciseAngina <- as.factor(train$ExerciseAngina)
train$ST_Slope <- as.factor(train$ST_Slope)
test$ChestPainType <- as.factor(test$ChestPainType)
test$Sex <- as.factor(test$Sex)
test$FastingBS <- as.factor(test$FastingBS)
test$RestingECG <- as.factor(test$RestingECG)
test$ExerciseAngina <- as.factor(test$ExerciseAngina)
test$ST_Slope <- as.factor(test$ST_Slope)
bagging <- randomForest(HeartDisease ~ .,data = train, mtry = 19, ntree = 500, type = "class", 
                        importance = TRUE,
                        proximity = TRUE) 
#Confusion Matrix
predict_bag <- predict(bagging, newdata = test)
predicted_bag <- ifelse(predict_bag>0.5,1,0)
confm_bag<-table(predicted_bag,test$HeartDisease)
#MSE
bagging_mse <- mean((predict_bag - test$HeartDisease)^2) 
#Over all correct:
overall_bag <- (confm_bag[1,1]+confm_bag[2,2])/sum(confm_bag)
kable(confm_bag) #confusion matrix
overall_bag #over all correct
#Visulizaton of bagging
VI <- data.frame(var=names(test[,-12]), imp=varImp(bagging))
#sort variable importance descending
#VI_plot <- VI[order(VI$imp.0, decreasing=TRUE),]
#visualize variable importance with horizontal bar plot
#barplot(VI_plot$imp.0,
   #     names.arg=rownames(VI_plot),
    #    horiz=TRUE,
     #   col='steelblue',
      #  xlab='Variable Importance')
ggplot(VI)+geom_col(aes(x=var,y=Overall,fill=var))+coord_flip()+guides(fill="none")+
  labs(x="Variable Name",y="Importance")
```


## Bosting



```{r,echo=FALSE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")


train$ChestPainType <- as.factor(train$ChestPainType)
train$Sex <- as.factor(train$Sex)
train$FastingBS <- as.factor(train$FastingBS)
train$RestingECG <- as.factor(train$RestingECG)
train$ExerciseAngina <- as.factor(train$ExerciseAngina)
train$ST_Slope <- as.factor(train$ST_Slope)


test$ChestPainType <- as.factor(test$ChestPainType)
test$Sex <- as.factor(test$Sex)
test$FastingBS <- as.factor(test$FastingBS)
test$RestingECG <- as.factor(test$RestingECG)
test$ExerciseAngina <- as.factor(test$ExerciseAngina)
test$ST_Slope <- as.factor(test$ST_Slope)



set.seed(445)




oob = trainControl(method = "oob")
cv_5 = trainControl(method = "cv", number = 5)

gbm_grid =  expand.grid(interaction.depth = 1:5,
                        n.trees = (1:5) * 200,
                        shrinkage = c(0.001,0.01,0.015,0.1),
                        n.minobsinnode = 10)


gbm_tune = train(as.factor(HeartDisease) ~ ., data = train,
                      method = "gbm",
                      trControl = cv_5,
                      verbose = FALSE,
                      tuneGrid = gbm_grid)
plot(gbm_tune)

gbm_tune$bestTune

``` 


In order to tune tree boosting method I used a 5 fold cross-validation on the training data set, from the plot it seems that there is a spike on the on the accuracy when using shrinkage with 0.01, 400 trees and a interaction depth of 2. It should also be noted that several other ranges of the tune parameters were tested but for simplicity sake on the range on the plot was left which included the best one overall.






```{r,echo=FALSE}
boost <- gbm(HeartDisease~.,data=train,n.trees=400,shrinkage=0.01,interaction.depth = 2 ,distribution = "bernoulli")


boostpred = ifelse(predict(boost,test, n.trees = 400, "response") > 0.5, "1", "0")

confboost<-table(predicted = boostpred, actual = test$HeartDisease)
confboost
error<-(confboost[1,1]+confboost[2,2])/sum(confboost)
overall_boost <- error
paste("accuraccy:",overall_boost)
```
Fitting the model with the tuning parameters previously discussed, we get a much better accuracy when compared to the other methods but at the cost of having lost the ability of doing inference in this data set. We can take a look at the importance graph below, in which we can see that ST_Slope, ChestPainType, and Cholesterol are the most important variables, which is somewhat consistent with most of the models used so far. Also Consistent with the other models, we can see that RestingECG and RestingBP are the least important variables for boosting.


```{r,include=FALSE}
summa<-summary(boost)
```




```{r,echo=FALSE}
ggplot(summa)+geom_col(aes(x=var,y=rel.inf,fill=var))+coord_flip()+labs(y="Relative Influence", x= " Variable")+guides(fill="none")
```

## Random Forest

```{r,echo=FALSE}
set.seed(445)

rf_fit <- randomForest(as.factor(HeartDisease) ~ ., data = train, mtry = sqrt(ncol(train)), importance = TRUE, type= "classification", ntrees=1000)

true <- test$HeartDisease
rf <- predict(rf_fit,test,type="class")



confm2<-table(predicted=rf,True=true)
confm2


paste(" Test Accuracy :",(confm2[1,1]+confm2[2,2])/sum(confm2))


overall_RF <- (confm2[1,1]+confm2[2,2])/sum(confm2)
rf_fit

imp<-data.frame(rf_fit$importance)

imp$variables <- rownames(imp)
rownames(imp) <- 1:nrow(imp)

imp <- imp[order(imp$MeanDecreaseAccuracy),]
```

Next, we fitted a Random Forest, in this case I chose the number of predictors randomly sampled at each iteration to be $m=\sqrt{p}$, as this was what was recommended by the class book. For random forests, the test accuracy relatively good at 0.8719 which is good, but not better than some of te other methods. Looking at the variable importance, we have some similar results to the other tree based methods, as ST_Slope, ChestPainType, and Cholesterol seem to be the most important variables. However, in random forests we got a negative value for the RestingBP, which is somewhat consistent with the other methos as RestingBP also did not seem to be a important variable in dertermining heart disease, however, in this case it means that for random forests is seems that including RestingBP decreases the accuracy of the method. 


```{r,echo=FALSE}
ggplot(imp)+geom_col(aes(x=variables,y=MeanDecreaseAccuracy,fill=variables))+coord_flip()+labs(title="Variable Importance",y=" Mean decrease in Accuracy", x= " Variable")+scale_y_continuous(labels = scales::percent_format())+guides(fill="none")
```



## Representative Tree

While working on the random forests, I began to  search for a way to visualize at least one of the trees in the ensemble, in order to more visualization and perhaps get a deeper insight into the method. While I found a way to do it in stack overflow, I also found a recommendation of a paper by Banerjee, et al. called "Identifying representative trees from ensembles" (2012), which was published in the journal of statistical medicine. What we found really interesting is that the example application which was very similar to our application as it was classification problem using kidney cancer data, and there they discuss the importance of not losing the ability to do inference while still having high accuracy.


In the paper Banerjee, et al. discuss how to choose the most representative tree in an ensemble, which is done by finding the average distance between one tree and all other trees in the ensemble. Banerjee, et al. discusses several types of distance in this paper only one of them was used, as it was already implemented in a R package. This measure captures the similarities between the predictions, in that 2 trees are similar if they have the same predictions for all subjects, this is defined by Banerjee, et al as :
$d_2(T_1,T_2)=\frac{1}{n}\sum_{i=1}^n(\hat{y}_{1i}-\hat{y}_{2i})^2$, if the trees perfectly similar, the distance $d_2(T_1,T_2)$ is equal 0. Finding the representative tree was done by using the package "reptree", and using it on this random forests we got the following tree:


```{r fig.height=7, fig.width=15, message=FALSE, warning=FALSE,echo=FALSE}
set.seed(445)

reptree <- ReprTree(rf_fit, train, metric='d2')

plot(reptree, index=1)

```




In the tree we can see ST_Slope is the root node, which was expected since most models considered it the most important variable, however, it is interesting that in the next nodes we have FastingBS and Sex:F both variables were not considered very important by the random forest model. This made us somewhat skeptical about this algorithm, however, it does make sense as we are looking for a representative tree not the most accuracate tree, if we had more time we would've liked to implement our own algorithm based of the paper in order to further validate the results.







## SVM

The last model we used was SVM. We wanted to using the tuning method to best optimize our SVM. We tested three kernals for our SVM: linear, polynomial, and radial. We tuned the cost parameter for each, the degree parameter for the polynomial kernal, and the gamma parameter for the radial kernal.

```{r SVM_linear,echo=FALSE}
svm_linear_cv = tune(svm, HeartDisease~., scale=TRUE, kernel="linear", data=train,
                      ranges = list(cost = seq(0.1, 5, by = 0.1)))

ggplot(data=svm_linear_cv$performances) + geom_point(aes(x=cost, y=error))
```

```{r SVM_poly,echo=FALSE}
svm_poly_cv = tune(svm, HeartDisease~., scale=TRUE, kernel="polynomial", data=train,
                      ranges = list(cost = seq(0.1, 5, by = 0.1),
                                    degree = c(2,3,4, 5)))

ggplot(data=svm_poly_cv$performances) + geom_point(aes(x=cost, y=error)) + 
  facet_grid(~degree) + labs(title="Error of Polynomial Kernal by Degree")
```

```{r SVM_radial,echo=FALSE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
train$ChestPainType <- as.factor(train$ChestPainType)
train$Sex <- as.factor(train$Sex)
train$FastingBS <- as.factor(train$FastingBS)
train$RestingECG <- as.factor(train$RestingECG)
train$ExerciseAngina <- as.factor(train$ExerciseAngina)
train$ST_Slope <- as.factor(train$ST_Slope)
test$ChestPainType <- as.factor(test$ChestPainType)
test$Sex <- as.factor(test$Sex)
test$FastingBS <- as.factor(test$FastingBS)
test$RestingECG <- as.factor(test$RestingECG)
test$ExerciseAngina <- as.factor(test$ExerciseAngina)
test$ST_Slope <- as.factor(test$ST_Slope)

SVM_radial = svm(HeartDisease~., data=train, kernel="radial", gamma=0.25, cost=0.4,type="C")

preds = predict(SVM_radial, test, type="class")
acc = sum(ifelse(preds == test$HeartDisease, 1, 0))/nrow(test)

table(preds,test$HeartDisease)

overall_SVM <- acc
```

We found that the radial kernel with cost of 0.4 and gamma parameter of 0.25 gave us the best training accuracy. Our testing accuracy for our SVM model was `r round(acc, 3)`.



# Conclusion

```{r,echo=FALSE}
library(knitr)
df1 = data.frame(Method = c("Logistic Regression","LDA","KNN","LASSO","Bagging","Random Forest","Boosting","SVM"),  Accuracy= c(overall_logit,overall_LDA,overall_knn_1,overall_lasso,overall_bag,overall_RF,overall_boost,overall_SVM))

kable(df1)

```

We found that all of our models, except KNN, had an accuracy within a few points as we can see from the table above. It was interesting to note from LDA that our prior probabilities of our two classes closely follow the CDCs probability of getting heart disease. ST-Slope was shown to be the most important predictor in most of our models and we estimate this is the reason for the similar accuracy between models. So choosing the best model depends a lot on whether or not we value more inference or prediction, if we only care about inference, boosting is probably the best method otherwise, something like the logistic regression might be more useful for inference. However, if both are really important a using a random forest with a representative tree might be the best way, it allows us to lose some predictive performance from boosting in order to gain the ability to do some interpretation.

## Future Proposal

It would be interesting to re-try this project, removing the ST-Slope value. As we learned from industry professionals, the ST Slope is nearly always associated with some type of heart rhythm anomaly, and thus is frequently present in cases of heart disease. Many of our models indicated the ST Slope as the number one predictor for heart disease due to this strong association. We also would've liked to be able to make more inference from our models. A model like LDA may have one of the highest accuracy, but we could make very little inference about our data from this model.


## References


Banerjee M, Ding Y, Noone AM. Identifying representative trees from ensembles. Stat Med. 2012 Jul 10;31(15):1601-16. doi: 10.1002/sim.4492. Epub 2012 Feb 3. PMID: 22302520.

